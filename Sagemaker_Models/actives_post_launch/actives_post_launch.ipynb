{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from lib.model import ModelMain\n",
    "\n",
    "# configs\n",
    "from lib.config import percent_data_process_info\n",
    "from lib.config import prelaunch_process_info\n",
    "from lib.config import metadata_process_info\n",
    "from lib.config import default_params_dict as params_dict\n",
    "from lib.config import model_name_list\n",
    "from lib.config import params_tunning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d49950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32a8a8",
   "metadata": {},
   "source": [
    "# Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f2b6b",
   "metadata": {},
   "source": [
    "Reading data from multiple sources, with the corresponding query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ecfb7",
   "metadata": {},
   "source": [
    "### Step 1 of the Prediction Process: Getting Data\n",
    "Step 1.1: update the funnel metrics by Sagemaker ipynb file 'query_pipeline' under the '/query' folder\n",
    "\n",
    "Step 1.2: run each of the query in the '/day28_prediction/query/' to extract each input csv below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e076676",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bucket = 'hbo-ingest-datascience-content-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baeee11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading funnel_metric_feature features\n",
      "Reading media_cost_postlaunch_feature features\n",
      "Reading media_cost_prelaunch_feature features\n",
      "Reading metadata_feature features\n",
      "Reading prelaunch_trailer_feature features\n",
      "Reading prelaunch_trailer_feature_before28 features\n",
      "Reading sub_total_feature features\n",
      "Reading trailer_feature features\n",
      "Reading vtp_feature features\n",
      "Reading wiki_view_feature_before28 features\n",
      "Reading wiki_view_post_feature features\n",
      "Reading wiki_view_pre_feature features\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.info(f'Loading inputs')\n",
    "data_list =[]\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(input_bucket)\n",
    "# Iterates through all the objects, doing the pagination for you. Each obj\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "# get to get the whole body.\n",
    "for obj in bucket.objects.filter(Prefix='input_percent_view'):\n",
    "    key = obj.key\n",
    "    logger.info('Loading csv file {}'.format(key))\n",
    "    body = obj.get()['Body']\n",
    "    var_name = key.split('.')[0].split('/')[1]\n",
    "    print('Reading {0} features'.format(var_name))\n",
    "    exec(\"{0}=pd.read_csv(body, na_values = [r'\\\\\\\\N'])\".format(var_name))\n",
    "    exec(\"{0}.columns = {0}.columns.str.lower()\".format(var_name))\n",
    "    \n",
    "    # exclude the full null columns\n",
    "    exec(\"{0} = {0}.loc[:,{0}.isnull().sum()!={0}.shape[0]]\".format(var_name))\n",
    "\n",
    "    # exclude the old Mortal Kombat movie because the trailer percent view \n",
    "    # matching matches the trailer of the new movie to the old movie\n",
    "    # exclude Tom & Jerry due to unresolvable data issue\n",
    "    exec(\"{0} = {0}.loc[{0}['match_id_platform'].\\\n",
    "        isin(['1-GYGQBcwsaCIW2XgEAAAAL', '0-GYGQBcwsaCIW2XgEAAAAL', '1-GYEb9QwLgFF9_ZwEAAAA7', '0-GYEb9QwLgFF9_ZwEAAAA7'])==False,:]\\\n",
    "        .reset_index(drop = True)\".format(var_name))\n",
    "    \n",
    "    # append the feature df\n",
    "    exec(\"data_list.append({0})\".format(var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2678271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pct_actives_metric_values features\n"
     ]
    }
   ],
   "source": [
    "bucket = s3.Bucket(input_bucket)\n",
    "for obj in bucket.objects.filter(Prefix='pct_actives_prediction/pct_actives_metric_values'):\n",
    "    key = obj.key\n",
    "    logger.info('Loading csv file {}'.format(key))\n",
    "    body = obj.get()['Body']\n",
    "    var_name = key.split('.')[0].split('/')[1]\n",
    "    print('Reading {0} features'.format(var_name))\n",
    "    exec(\"{0}=pd.read_csv(body, na_values = [r'\\\\\\\\N'])\".format(var_name))\n",
    "    exec(\"{0}.columns = {0}.columns.str.lower()\".format(var_name))\n",
    "    \n",
    "    # exclude the full null columns\n",
    "    exec(\"{0} = {0}.loc[:,{0}.isnull().sum()!={0}.shape[0]]\".format(var_name))\n",
    "\n",
    "    # exclude the old Mortal Kombat movie because the trailer percent view \n",
    "    # matching matches the trailer of the new movie to the old movie\n",
    "    # exclude Tom & Jerry due to unresolvable data issue\n",
    "    exec(\"{0} = {0}.loc[{0}['match_id'].\\\n",
    "        isin(['1-GYGQBcwsaCIW2XgEAAAAL', '0-GYGQBcwsaCIW2XgEAAAAL', '1-GYEb9QwLgFF9_ZwEAAAA7', '0-GYEb9QwLgFF9_ZwEAAAA7'])==False,:]\\\n",
    "        .reset_index(drop = True)\".format(var_name))\n",
    "    \n",
    "    # append the feature df\n",
    "    exec(\"data_list.append({0})\".format(var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f3b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_feature = data_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0194ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data = data_list[-1][['match_id', 'days_on_hbo_max', 'pct_actives']]\n",
    "active_data = active_data.merge(metadata_feature[['match_id', 'match_id_platform']], on = 'match_id')\n",
    "active_data.drop(['match_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7eb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data['pct_actives_values'] = active_data.groupby(['match_id_platform', 'days_on_hbo_max'])['pct_actives'].transform('mean')\n",
    "active_data = active_data[['match_id_platform', 'days_on_hbo_max', 'pct_actives_values']]\n",
    "active_data = active_data[(active_data['match_id_platform'].notnull())\n",
    "                         &(active_data['days_on_hbo_max'].notnull())]\n",
    "active_data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746ea45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data = active_data.pivot(index='match_id_platform', columns='days_on_hbo_max', values=['pct_actives_values']).reset_index()\n",
    "columns = ['day00' + str(i) + '_percent_actives' for i in range(1, 10) ]\n",
    "columns = columns + ['day0' + str(i) + '_percent_actives' for i in range(10, 29)]\n",
    "active_data.columns = ['match_id_platform'] + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49df6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list.pop(-1)\n",
    "data_list.append(active_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761443b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final title size: 8052, All title size: 8052\n"
     ]
    }
   ],
   "source": [
    "# start a object\n",
    "logger.info('Setting up the prediction model')\n",
    "percentile_used = 0.8\n",
    "back_consideration_date = 180\n",
    "nfold = np.floor(back_consideration_date/30)\n",
    "cv_func = ModelMain(data_list, metadata_process_info['label_columns'], metadata_process_info['num_columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d041d",
   "metadata": {},
   "source": [
    "# New Title Prediction, Post Launch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c43d96",
   "metadata": {},
   "source": [
    "### Cross Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1416 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -27\n",
      "SMAPE for all titles 0.6229156161907226\n",
      "SMAPE for the originals 0.6865927899132753\n",
      "running cvs at day -27\n",
      "Making prediction for day -27\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1416 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -26\n",
      "SMAPE for all titles 0.6431574958552408\n",
      "SMAPE for the originals 0.686821528926303\n",
      "running cvs at day -26\n",
      "Making prediction for day -26\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1418 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -25\n",
      "SMAPE for all titles 0.6414275892250167\n",
      "SMAPE for the originals 0.6873337027935292\n",
      "running cvs at day -25\n",
      "Making prediction for day -25\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1418 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -24\n",
      "SMAPE for all titles 0.6418802240779375\n",
      "SMAPE for the originals 0.6957249698876844\n",
      "running cvs at day -24\n",
      "Making prediction for day -24\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1418 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -23\n",
      "SMAPE for all titles 0.6343497158983493\n",
      "SMAPE for the originals 0.6984087857142132\n",
      "running cvs at day -23\n",
      "Making prediction for day -23\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1419 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -22\n",
      "SMAPE for all titles 0.6317233856130258\n",
      "SMAPE for the originals 0.6846359488754432\n",
      "running cvs at day -22\n",
      "Making prediction for day -22\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1423 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -21\n",
      "SMAPE for all titles 0.6316487242341718\n",
      "SMAPE for the originals 0.6836351882219277\n",
      "running cvs at day -21\n",
      "Making prediction for day -21\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1424 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -20\n",
      "SMAPE for all titles 0.6280466107269183\n",
      "SMAPE for the originals 0.6831000665943634\n",
      "running cvs at day -20\n",
      "Making prediction for day -20\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1424 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -19\n",
      "SMAPE for all titles 0.6382375735929341\n",
      "SMAPE for the originals 0.7345487723499308\n",
      "running cvs at day -19\n",
      "Making prediction for day -19\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1423 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -18\n",
      "SMAPE for all titles 0.6384318391867617\n",
      "SMAPE for the originals 0.7303338985200055\n",
      "running cvs at day -18\n",
      "Making prediction for day -18\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1423 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -17\n",
      "SMAPE for all titles 0.638280188605111\n",
      "SMAPE for the originals 0.7147177573029653\n",
      "running cvs at day -17\n",
      "Making prediction for day -17\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1426 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -16\n",
      "SMAPE for all titles 0.6377966374698358\n",
      "SMAPE for the originals 0.7126094783399469\n",
      "running cvs at day -16\n",
      "Making prediction for day -16\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1424 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -15\n",
      "SMAPE for all titles 0.6389279122635624\n",
      "SMAPE for the originals 0.71641814273612\n",
      "running cvs at day -15\n",
      "Making prediction for day -15\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1427 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -14\n",
      "SMAPE for all titles 0.6405250931967431\n",
      "SMAPE for the originals 0.720236823511582\n",
      "running cvs at day -14\n",
      "Making prediction for day -14\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1429 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -13\n",
      "SMAPE for all titles 0.6384735230568959\n",
      "SMAPE for the originals 0.6947421263009319\n",
      "running cvs at day -13\n",
      "Making prediction for day -13\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1430 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -12\n",
      "SMAPE for all titles 0.6500974537970233\n",
      "SMAPE for the originals 0.7653228178782199\n",
      "running cvs at day -12\n",
      "Making prediction for day -12\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1432 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -11\n",
      "SMAPE for all titles 0.6522477772446583\n",
      "SMAPE for the originals 0.7691524317076227\n",
      "running cvs at day -11\n",
      "Making prediction for day -11\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1431 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -10\n",
      "SMAPE for all titles 0.6546911677198454\n",
      "SMAPE for the originals 0.7755107212355833\n",
      "running cvs at day -10\n",
      "Making prediction for day -10\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1431 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -9\n",
      "SMAPE for all titles 0.6653105532830498\n",
      "SMAPE for the originals 0.8539477716708769\n",
      "running cvs at day -9\n",
      "Making prediction for day -9\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1434 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -8\n",
      "SMAPE for all titles 0.6653820432715959\n",
      "SMAPE for the originals 0.8583178495410719\n",
      "running cvs at day -8\n",
      "Making prediction for day -8\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1436 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -7\n",
      "SMAPE for all titles 0.6470474499866334\n",
      "SMAPE for the originals 0.8448659310926936\n",
      "running cvs at day -7\n",
      "Making prediction for day -7\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1435 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -6\n",
      "SMAPE for all titles 0.6463313148142013\n",
      "SMAPE for the originals 0.8951988878748826\n",
      "running cvs at day -6\n",
      "Making prediction for day -6\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1436 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -5\n",
      "SMAPE for all titles 0.6412869561517262\n",
      "SMAPE for the originals 0.8855737754602439\n",
      "running cvs at day -5\n",
      "Making prediction for day -5\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1436 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -4\n",
      "SMAPE for all titles 0.6439221170958698\n",
      "SMAPE for the originals 0.907340193911351\n",
      "running cvs at day -4\n",
      "Making prediction for day -4\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1438 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -3\n",
      "SMAPE for all titles 0.643049687533382\n",
      "SMAPE for the originals 0.8953848759083898\n",
      "running cvs at day -3\n",
      "Making prediction for day -3\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1437 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -2\n",
      "SMAPE for all titles 0.64324996796906\n",
      "SMAPE for the originals 0.8995729790531842\n",
      "running cvs at day -2\n",
      "Making prediction for day -2\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1438 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day -1\n",
      "SMAPE for all titles 0.6426612590770951\n",
      "SMAPE for the originals 0.9024712030316536\n",
      "running cvs at day -1\n",
      "Making prediction for day -1\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "only 1442 titles considered after prelaunch filter\n",
      "the number of days is not large enough to use log ratio transformation\n",
      "X and y are ready based on the input params\n",
      "Do cross prediction for day 0\n",
      "SMAPE for all titles 0.6365128682152532\n",
      "SMAPE for the originals 0.891087965461211\n",
      "running cvs at day 0\n",
      "Making prediction for day 0\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "X and y are ready based on the input params\n",
      "Tune parameter for day 1\n",
      "parameter combination 1\n",
      "parameter combination 2\n",
      "parameter combination 3\n",
      "parameter combination 4\n",
      "parameter combination 5\n",
      "parameter combination 6\n",
      "parameter combination 7\n",
      "parameter combination 8\n",
      "SMAPE for all titles 0.2814168202209212\n",
      "SMAPE for the originals 0.3101249419784231\n",
      "running cvs at day 1\n",
      "Making prediction for day 1\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "X and y are ready based on the input params\n",
      "Tune parameter for day 2\n",
      "parameter combination 1\n",
      "parameter combination 2\n",
      "parameter combination 3\n",
      "parameter combination 4\n",
      "parameter combination 5\n",
      "parameter combination 6\n",
      "parameter combination 7\n",
      "parameter combination 8\n",
      "SMAPE for all titles 0.263922736375268\n",
      "SMAPE for the originals 0.2677570332869529\n",
      "running cvs at day 2\n",
      "Making prediction for day 2\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "X and y are ready based on the input params\n",
      "Tune parameter for day 3\n",
      "parameter combination 1\n",
      "parameter combination 2\n",
      "parameter combination 3\n",
      "parameter combination 4\n",
      "parameter combination 5\n",
      "parameter combination 6\n",
      "parameter combination 7\n",
      "parameter combination 8\n",
      "SMAPE for all titles 0.2546020884584887\n",
      "SMAPE for the originals 0.26116040113402766\n",
      "running cvs at day 3\n",
      "Making prediction for day 3\n",
      "keeps the titles above 80.0 percentile day1 viewed over all titles only\n",
      "only 1909 titles considered after popularity filter\n",
      "X and y are ready based on the input params\n",
      "Tune parameter for day 4\n",
      "parameter combination 1\n",
      "parameter combination 2\n",
      "parameter combination 3\n"
     ]
    }
   ],
   "source": [
    "percent_data_process_info['exact_X_pred'] = False\n",
    "output_flag = True\n",
    "new_title_output = pd.DataFrame()\n",
    "existing_title_output = pd.DataFrame()\n",
    "back_consideration_date = 180\n",
    "\n",
    "for day in range(-27,28):\n",
    "    # renew the percent_data_process_info data very time\n",
    "    from lib.config import percent_data_process_info\n",
    "    from lib.config import prelaunch_process_info\n",
    "    from lib.config import metadata_process_info\n",
    "\n",
    "    # determine prelaunch or postlaunch\n",
    "    if day < 1:\n",
    "        input_process_info = dict(prelaunch_process_info)\n",
    "        percent_data_process_info['target_log_transformation'] = False\n",
    "        percent_data_process_info['log_ratio_transformation'] = False\n",
    "        input_percentile_used = percentile_used\n",
    "        model_name = 'lr'\n",
    "        model_name_list = [model_name]\n",
    "    elif day<14:\n",
    "        input_process_info = dict(metadata_process_info)\n",
    "        percent_data_process_info['target_log_transformation'] = True\n",
    "        percent_data_process_info['log_ratio_transformation'] = True\n",
    "        input_percentile_used = percentile_used\n",
    "        model_name = 'lgb'\n",
    "        model_name_list = [model_name]\n",
    "    else:\n",
    "        input_process_info = dict(metadata_process_info)\n",
    "        percent_data_process_info['target_log_transformation'] = False\n",
    "        percent_data_process_info['log_ratio_transformation'] = False\n",
    "        input_percentile_used = percentile_used\n",
    "        model_name = 'lr'\n",
    "        model_name_list = [model_name]\n",
    "\n",
    "    # just to make the values in the dict back to the initial values\n",
    "    percent_data_process_info = dict(percent_data_process_info)\n",
    "    percent_data_process_info['max_num_day'] = day\n",
    "    \n",
    "    # get x and y\n",
    "    logger.info('Get X and y for day {}'.format(day))\n",
    "    cv_func.get_X_y(percent_data_process_info, \n",
    "                     input_process_info, \n",
    "                     day001_popularity_threshold = input_percentile_used)\n",
    "\n",
    "    # tune parameter\n",
    "    if model_name not in  ['lr', 'enet']:\n",
    "        logger.info('Tune parameter for day {}'.format(day))\n",
    "        print('Tune parameter for day {}'.format(day))\n",
    "        cv_func.parameter_tuning(model_name, \n",
    "                            params_tunning_dict, \n",
    "                            percent_data_process_info,\n",
    "                            nfold = nfold,\n",
    "                            back_consideration_date = back_consideration_date)\n",
    "        \n",
    "        params_dict = cv_func.min_smape_param['min_smape_original']\n",
    "        param_stats = cv_func.parameter_tuning_stats\n",
    "        logger.info('SMAPE for all titles {}'.format(param_stats['min_smape_all']))\n",
    "        logger.info('SMAPE for the originals {}'.format(param_stats['min_smape_original']))\n",
    "        print('SMAPE for all titles {}'.format(param_stats['min_smape_all']))\n",
    "        print('SMAPE for the originals {}'.format(param_stats['min_smape_original']))\n",
    "    \n",
    "    else:\n",
    "        logger.info('Do cross prediction for day {}'.format(day))\n",
    "        print('Do cross prediction for day {}'.format(day))\n",
    "        cv_func.cross_prediction(\n",
    "                         model_name_list, \n",
    "                         params_dict, \n",
    "                         percent_data_process_info, \n",
    "                         nfold = nfold, \n",
    "                         back_consideration_date = back_consideration_date)\n",
    "        \n",
    "        logger.info('SMAPE for all titles {}'.format(cv_func.output['smape_' + model_name].mean()))\n",
    "        logger.info('SMAPE for the originals {}'.format(cv_func.output.loc[cv_func.output['program_type']==1,'smape_' + model_name].mean()))\n",
    "        print('SMAPE for all titles {}'.format(cv_func.output['smape_' + model_name].mean()))\n",
    "        print('SMAPE for the originals {}'.format(cv_func.output.loc[cv_func.output['program_type']==1,'smape_' + model_name].mean()))\n",
    "    \n",
    "    # make prediction\n",
    "    print('running cvs at day {}'.format(day))\n",
    "    cur_existing_title_output = cv_func.output\n",
    "    pred_column = cur_existing_title_output.columns[cur_existing_title_output.columns.str.contains(model_name)][0]\n",
    "    cur_existing_title_output['pred_day'] = day\n",
    "    cur_existing_title_output = cur_existing_title_output.rename(columns = {pred_column:'prediction'})\n",
    "    cur_existing_title_output = cur_existing_title_output.rename(columns = {'smape_lgb':'smape'\n",
    "                                                                    ,'smape_lr':'smape'\n",
    "                                                                    ,'smape_enet':'smape'\n",
    "                                                                    ,'mae_lgb':'mae'\n",
    "                                                                    ,'mae_lr':'mae'\n",
    "                                                                    ,'mae_enet':'mae'\n",
    "                                                                    })\n",
    "    existing_title_output = pd.concat([existing_title_output, cur_existing_title_output], axis = 0)\n",
    "            \n",
    "    if cv_func.pred_empty_flag == True:\n",
    "        pass\n",
    "    else:\n",
    "        logger.info('Making prediction for day {}'.format(day))\n",
    "        print('Making prediction for day {}'.format(day))\n",
    "        cv_func.predict_new_titles(model_name_list, \n",
    "                                   params_dict, \n",
    "                                   percent_data_process_info)\n",
    "    \n",
    "        # process the output\n",
    "        cur_new_title_output = cv_func.new_title_output\n",
    "        pred_column = cur_new_title_output.columns[cur_new_title_output.columns.str.contains(model_name)][0]\n",
    "        cur_new_title_output['pred_day'] = day\n",
    "        cur_new_title_output = cur_new_title_output.rename(columns = {pred_column:'prediction'})\n",
    "\n",
    "        new_title_output = pd.concat([new_title_output,cur_new_title_output], axis = 0)\n",
    "#         print (new_title_output.head())\n",
    "\n",
    "          \n",
    "# final formatting\n",
    "\n",
    "if new_title_output.shape[0]>0:    \n",
    "    new_title_output = new_title_output.drop(columns = ['target']).sort_values(['title_name','pred_day'])\n",
    "    new_title_output = new_title_output[['title_name'\n",
    "                                        ,'match_id'\n",
    "                                        ,'match_id_platform'\n",
    "                                        ,'platform_name'\n",
    "                                        ,'program_type'\n",
    "                                        ,'pred_day'\n",
    "                                        ,'prediction']]\n",
    "\n",
    "if existing_title_output.shape[0]>0:           \n",
    "    existing_title_output = existing_title_output.sort_values(['match_id_platform','pred_day'])\n",
    "    existing_title_output['platform_name'] = existing_title_output['match_id_platform'].apply(lambda x: x[0])\n",
    "    existing_title_output = existing_title_output[['title_name'\n",
    "                                                ,'match_id'\n",
    "                                                ,'match_id_platform'\n",
    "                                                ,'platform_name'\n",
    "                                                ,'program_type'\n",
    "                                                ,'target'\n",
    "                                                ,'pred_day'\n",
    "                                                ,'prediction'\n",
    "                                                ,'smape'\n",
    "                                                ,'mae'\n",
    "                                                ,'fold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0193c2",
   "metadata": {},
   "source": [
    "# Write csvs to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22681dbd",
   "metadata": {},
   "source": [
    "### Step 3: Write the prediction result to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523782b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bucket = 'hbo-outbound-datascience-content-dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3(filename, output_bucket, content):\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=filename, Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing new title predictions over time to S3 as an csv file')\n",
    "print('Writing new title predictions over time to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "new_title_output.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'output_percent_actives/new_title_prediction.csv'\n",
    "\n",
    "to_s3(filename, output_bucket, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6acd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing existing title predictions over time to S3 as an csv file')\n",
    "print('Writing existing title predictions over time to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "existing_title_output.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'output_percent_actives/existing_title_prediction.csv'\n",
    "\n",
    "to_s3(filename, output_bucket, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22f0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
