{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import boto3\n",
    "import sys\n",
    "import datetime\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from xgboost.sklearn import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DATE: str = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from multiple sources, with the corresponding query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 of the Prediction Process: Getting Data\n",
    "Step 1.1: update the funnel metrics by Sagemaker ipynb file 'query_pipeline' under the '/query' folder\n",
    "\n",
    "Step 1.2: run each of the query in the '/day28_prediction/query/' to extract each input csv below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.info(f'Loading inputs')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(input_bucket)\n",
    "\n",
    "data_list = []\n",
    "# Iterates through all the objects, doing the pagination for you. Each obj\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "# get to get the whole body.\n",
    "for obj in bucket.objects.filter(Prefix='hourspct'):\n",
    "    key = obj.key\n",
    "    if 'sagemaker' not in key:\n",
    "        logger.info('Loading csv file {}'.format(key))\n",
    "        body = obj.get()['Body']\n",
    "        var_name = key.split('.')[0].split('/')[1]\n",
    "        print('Reading {0} features'.format(var_name))\n",
    "        exec(\"{0}=pd.read_csv(body, na_values = [r'\\\\\\\\N'])\".format(var_name))\n",
    "        exec(\"{0}.columns = {0}.columns.str.lower()\".format(var_name))\n",
    "        \n",
    "        # exclude the full null columns\n",
    "        exec(\"{0} = {0}.loc[:,{0}.isnull().sum()!={0}.shape[0]]\".format(var_name))\n",
    "        \n",
    "        # append the feature df\n",
    "        exec(\"data_list.append({0})\".format(var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(len(data_list)):\n",
    "    data_list[cnt].columns = data_list[cnt].columns.str.lower().str.replace(\"'\", \"\")\n",
    "\n",
    "df = data_list[0]\n",
    "\n",
    "for data in data_list[1:]:\n",
    "    df = df.merge(data, how = 'outer', on = 'match_id')\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "df = df.loc[~df.match_id.duplicated()]\n",
    "df = df.set_index('match_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use shows with at least 14d history\n",
    "df = df.loc[~(df.hourspct14d.isnull() | df.hourspct7d.isnull())]\n",
    "df['max_window'] = 14\n",
    "df.loc[~df.hourspct21d.isnull(), 'max_window'] = 21\n",
    "df.loc[~df.hourspct28d.isnull(), 'max_window'] = 28\n",
    "df.loc[~df.hourspct8w.isnull(), 'max_window'] = 56\n",
    "df.loc[~df.hourspct13w.isnull(), 'max_window'] = 91\n",
    "df.loc[~df.hourspct26w.isnull(), 'max_window'] = 182\n",
    "\n",
    "df = df.drop(columns=['licensor_agg',\n",
    "                     'match_id_platform',\n",
    "                     'navigation_genre_desc_agg',\n",
    "                     'descriptive_genre_desc_agg',\n",
    "                     'title_name']\n",
    "            ,errors='ignore')\n",
    "\n",
    "df['wm_enterprise_genres_agg'] = df.wm_enterprise_genres_agg.fillna('nan').apply(lambda x: x.replace(' |', '|').replace('| ', '|'))\n",
    "df['launch_time'] = (pd.to_datetime(df['earliest_offered_timestamp']) - pd.to_datetime('2020-05-27')).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logit_hourspct7d'] = logit(df['hourspct7d']/100.0)\n",
    "df['logit_hourspct14d'] = logit(df['hourspct14d']/100.0)\n",
    "df['logit_hourspct21d'] = logit(df['hourspct21d']/100.0)\n",
    "df['logit_hourspct28d'] = logit(df['hourspct28d']/100.0)\n",
    "df['logit_hourspct8w'] = logit(df['hourspct8w']/100.0)\n",
    "df['logit_hourspct13w'] = logit(df['hourspct13w']/100.0)\n",
    "df['logit_hourspct26w'] = logit(df['hourspct26w']/100.0)\n",
    "df['logit_hourspct52w'] = logit(df['hourspct52w']/100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glist = set()\n",
    "for item in list(df.wm_enterprise_genres_agg):\n",
    "    glist = glist | set(item.split('|'))\n",
    "if '' in glist:\n",
    "    glist.remove('')\n",
    "genre_cols = pd.DataFrame(index=df.index, columns=glist)\n",
    "\n",
    "\n",
    "for match_id in df.index:\n",
    "    genres = df.loc[match_id, 'wm_enterprise_genres_agg'].split('|')\n",
    "    if '' in genres:\n",
    "        genres.remove('')\n",
    "    genre_cols.loc[match_id, genres] = 1\n",
    "    \n",
    "genre_cols = genre_cols.add_prefix('genre_').fillna(0)\n",
    "df = pd.concat([df, genre_cols], axis=1).drop(columns='wm_enterprise_genres_agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.program_type=='unknown', 'program_type'] = np.nan\n",
    "df.loc[df.program_type=='acquired', 'program_type'] = 0\n",
    "df.loc[df.program_type=='original', 'program_type'] = 1\n",
    "\n",
    "#df.loc[df.content_category=='special', 'content_category'] = 0\n",
    "#df.loc[df.content_category=='movies', 'content_category'] = 0\n",
    "df.loc[df.content_category!='series', 'content_category'] = 0\n",
    "df.loc[df.content_category=='series', 'content_category'] = 1\n",
    "\n",
    "\n",
    "df = df.rename(columns={'program_type': 'is_original', 'content_category': 'is_series'})\n",
    "df.is_original = df.is_original.astype('float')\n",
    "df.is_series = df.is_series.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fulldata = df.loc[~df.hourspct52w.isnull()]\n",
    "df_fulldata = df_fulldata.drop(df_fulldata.loc[df_fulldata.hourspct52w==0].index)\n",
    "logger.info(f'df columns: {list(df_fulldata.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random cutting of hourspct values for model to learn null values\n",
    "# 10% set as null\n",
    "\n",
    "# only works in numpy >= 1.17\n",
    "#rng = np.random.default_rng(15)\n",
    "#df_mask = pd.DataFrame(rng.choice([0,np.nan], size=[len(df_fulldata),5], p=[0.9,0.1]),\n",
    "\n",
    "np.random.seed(242)\n",
    "df_mask = pd.DataFrame(np.random.choice([0,np.nan], size=[len(df_fulldata),5], p=[0.9,0.1]),\n",
    "                       columns=['logit_hourspct21d',\n",
    "                                'logit_hourspct28d',\n",
    "                                'logit_hourspct8w',\n",
    "                                'logit_hourspct13w',\n",
    "                                'logit_hourspct26w'],\n",
    "                       index=df_fulldata.index)\n",
    "\n",
    "df_fulldata.loc[:,['logit_hourspct21d','logit_hourspct28d','logit_hourspct8w','logit_hourspct13w','logit_hourspct26w']] += df_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcols = ['logit_hourspct7d',\n",
    "         'logit_hourspct14d',\n",
    "         'logit_hourspct21d',\n",
    "         'logit_hourspct28d',\n",
    "         'logit_hourspct8w',\n",
    "         'logit_hourspct13w',\n",
    "         'logit_hourspct26w',\n",
    "         'season_number_adj',\n",
    "         'is_original',\n",
    "         'is_series',\n",
    "         'single_episode_ind',\n",
    "         'in_sequantial_releasing_period',\n",
    "         'at_release_year',\n",
    "         'dayofweek_earliest_date',\n",
    "         'total_hours',\n",
    "         'prod_release_year',\n",
    "         'title_age_approx',\n",
    "         'content_cost',\n",
    "         'ln_total_media_cost_pre_launch',\n",
    "         'min_days_since_offered',\n",
    "         'launch_time',\n",
    "         'genre_sports',\n",
    "         'genre_romance',\n",
    "         'genre_news/talk',\n",
    "         'genre_music',\n",
    "         'genre_drama',\n",
    "         'genre_crime',\n",
    "         'genre_latino',\n",
    "         'genre_originals',\n",
    "         'genre_suspense',\n",
    "         'genre_comedy',\n",
    "         'genre_documentary',\n",
    "         'genre_action',\n",
    "         'genre_international',\n",
    "         'genre_reality',\n",
    "         'genre_nan',\n",
    "         'genre_fantasy & sci-fi',\n",
    "         'genre_shorts',\n",
    "         'genre_horror'\n",
    "        ]\n",
    "ycol = ['logit_hourspct52w']\n",
    "trainx,testx,trainy,testy = train_test_split(df_fulldata[xcols], df_fulldata[ycol], test_size=0.2, random_state=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'reg:squarederror'\n",
    "         , 'n_estimators': 200\n",
    "         , 'min_child_weight': 5\n",
    "         , 'max_depth': 5\n",
    "         , 'learning_rate': 0.3\n",
    "         , 'gamma': 0.3\n",
    "         , 'colsample_bytree': 1.0}\n",
    "\n",
    "gridspace = {'objective': [ 'reg:squarederror' ]\n",
    "             ,\"learning_rate\" : [ 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ]\n",
    "             ,\"max_depth\" : [ 2, 3, 4, 5, 6, 8, 10 ]\n",
    "             , \"min_child_weight\" : [ 1, 3, 5, 7 ]\n",
    "             , \"gamma\" : [ 0.0, 0.1, 0.2 , 0.3, 0.4, 0.5 ]\n",
    "             , \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 1.0 ] }\n",
    "testgrid = {\n",
    "    \"max_depth\": [2,3]\n",
    "    ,'objective': ['reg:squarederror']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor()\n",
    "\n",
    "xgb_grid = RandomizedSearchCV(xgbr,\n",
    "                        gridspace,\n",
    "                        cv = 10,\n",
    "                        n_iter = 100,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(trainx, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_52w = expit(xgb_grid.best_estimator_.predict(df.loc[df.hourspct52w.isnull(), xcols]))*100\n",
    "\n",
    "df_pred_52w = pd.DataFrame(pred_52w, index = df.loc[df.hourspct52w.isnull()].index)\n",
    "df_pred_52w.columns = ['predicted_hourspct']\n",
    "#TODO df formatting for table\n",
    "\n",
    "df_pred_52w['prediction_date'] = TARGET_DATE\n",
    "df_pred_52w['window_days'] = df.loc[df_pred_52w.index, 'max_window']\n",
    "df_pred_52w = df_pred_52w.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write csvs to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write the prediction result to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3(filename, output_bucket, content):\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key='hourspct/'+filename, Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing hours% predictions to S3 as an csv file')\n",
    "print('Writing hours% predictions to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "df_pred_52w.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'new_hourspct_prediction.csv'\n",
    "\n",
    "to_s3(filename, output_bucket, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
