{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "from lib.model import ModelMain\n",
    "\n",
    "# configs\n",
    "from lib.config import percent_data_process_info\n",
    "from lib.config import metadata_process_info\n",
    "from lib.config import default_params_dict as params_dict\n",
    "from lib.config import model_name_list\n",
    "from lib.config import params_tunning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from multiple sources, with the corresponding query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 of the Prediction Process: Getting Data\n",
    "Step 1.1: update the funnel metrics by Sagemaker ipynb file 'query_pipeline' under the '/query' folder\n",
    "\n",
    "Step 1.2: run each of the query in the '/day28_prediction/query/' to extract each input csv below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.info(f'Loading inputs')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('hbo-ingest-datascience-content-dev')\n",
    "# Iterates through all the objects, doing the pagination for you. Each obj\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "# get to get the whole body.\n",
    "for obj in bucket.objects.all():\n",
    "    key = obj.key\n",
    "    if 'sagemaker' not in key:\n",
    "        logger.info('Loading csv file {}'.format(key))\n",
    "        body = obj.get()['Body']\n",
    "        var_name = key.split('.')[0]\n",
    "        exec(\"{}=pd.read_csv(body, na_values = [r'\\\\\\\\N'])\".format(var_name))\n",
    "        exec(\"{}.columns = {}.columns.str.lower()\".format(var_name, var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_metric_feature = funnel_metric_feature.loc[:,funnel_metric_feature.isnull().sum()!=funnel_metric_feature.shape[0]]\n",
    "vtp_feature = vtp_feature.loc[:,vtp_feature.isnull().sum()!=vtp_feature.shape[0]]\n",
    "sub_total_feature = sub_total_feature.loc[:,sub_total_feature.isnull().sum()!=sub_total_feature.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_used = 0.8\n",
    "data_list = [funnel_metric_feature, \n",
    "             metadata_feature, \n",
    "             mp_click_prelaunch_feature, \n",
    "             trailer_feature, \n",
    "             sub_total_feature,\n",
    "             vtp_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a object\n",
    "logger.info('Setting up the prediction model')\n",
    "nfold = 10\n",
    "cv_func = ModelMain(data_list, metadata_process_info['label_columns'], metadata_process_info['num_columns'])\n",
    "# movie only\n",
    "#cv_func.df = cv_func.df.loc[cv_func.df['content_category']=='movies',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Title Prediction, Post Launch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Prediction\n",
    "Note: Because it is a post launch prediction, only the titles with partial percent view and view through portion data will be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the prediction tarjectory over length of data\n",
    "'''\n",
    "\n",
    "model_name = 'lgb'\n",
    "model_name_list = ['lgb']\n",
    "percent_data_process_info['exact_X_pred'] = False\n",
    "output_flag = False\n",
    "percent_data_process_info_copy = dict(percent_data_process_info)\n",
    "\n",
    "for day in [1,2,3]:\n",
    "    # renew the percent_data_process_info data very time\n",
    "    from lib.config import percent_data_process_info\n",
    "    # just to make the values in the dict back to the initial values\n",
    "    percent_data_process_info = dict(percent_data_process_info_copy)\n",
    "    percent_data_process_info['max_num_day'] = day\n",
    "    \n",
    "    # get x and y\n",
    "    logger.info('Get X and y for day {}'.format(day))\n",
    "    cv_func.get_X_y(percent_data_process_info, \n",
    "                     metadata_process_info, \n",
    "                     day001_popularity_threshold = percentile_used)\n",
    "    # tune parameter\n",
    "    logger.info('Tune parameter for day {}'.format(day))\n",
    "    cv_func.parameter_tunning(model_name, \n",
    "                          params_tunning_dict, \n",
    "                          percent_data_process_info,\n",
    "                          nfold = nfold)\n",
    "    params_dict = cv_func.min_smape_param['min_smape_original']\n",
    "    param_stats = cv_func.parameter_tunning_stats\n",
    "    logger.info('SMAPE for all titles {}'.format(param_stats['min_smape_all']))\n",
    "    logger.info('SMAPE for the originals {}'.format(param_stats['min_smape_original']))\n",
    "    \n",
    "    # make prediction\n",
    "    logger.info('Making prediction for day {}'.format(day))\n",
    "    cv_func.predict_new_titles(model_name_list, \n",
    "                               params_dict, \n",
    "                               percent_data_process_info)\n",
    "    \n",
    "    if output_flag:\n",
    "        new_title_output = new_title_output.merge(cv_func.new_title_output, \n",
    "                                                  how = 'outer', \n",
    "                                                  on = ['title_name', 'match_id', 'target', 'program_type'])\n",
    "    else:\n",
    "        new_title_output = cv_func.new_title_output\n",
    "        output_flag = True\n",
    "        \n",
    "new_title_output = new_title_output.drop(columns = ['target', 'program_type']).sort_values('day_1_lgb', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write csvs to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write the prediction result to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3(filename, content):\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket='hbo-outbound-datascience-content-dev', Key=filename, Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing prediction over time to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "new_title_output.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'prediction_over_time.csv'\n",
    "\n",
    "to_s3(filename, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pred = new_title_output.loc[:,new_title_output.columns.str.contains('lgb')].ffill(axis=1).iloc[:,-1]\n",
    "last_pred = new_title_output[['title_name']].merge(last_pred, left_index = True, right_index = True).rename(columns = {new_title_output.columns[-1]:'last_pred'})\n",
    "last_pred = last_pred.sort_values('last_pred', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing past prediction to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "last_pred.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'last_prediction.csv'\n",
    "\n",
    "to_s3(filename, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
