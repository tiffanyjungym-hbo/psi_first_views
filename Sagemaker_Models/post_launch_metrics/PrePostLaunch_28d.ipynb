{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "from lib.model import ModelMain\n",
    "\n",
    "# configs\n",
    "from lib.config import percent_data_process_info\n",
    "from lib.config import prelaunch_process_info\n",
    "from lib.config import metadata_process_info\n",
    "from lib.config import default_params_dict as params_dict\n",
    "from lib.config import model_name_list\n",
    "from lib.config import params_tuning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from multiple sources, with the corresponding query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 of the Prediction Process: Getting Data\n",
    "Step 1.1: update the funnel metrics by Sagemaker ipynb file 'query_pipeline' under the '/query' folder\n",
    "\n",
    "Step 1.2: run each of the query in the '/day28_prediction/query/' to extract each input csv below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.info(f'Loading inputs')\n",
    "data_list =[]\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(input_bucket)\n",
    "# Iterates through all the objects, doing the pagination for you. Each obj\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "# get to get the whole body.\n",
    "for obj in bucket.objects.filter(Prefix='input_percent_view'):\n",
    "    key = obj.key\n",
    "    logger.info('Loading csv file {}'.format(key))\n",
    "    body = obj.get()['Body']\n",
    "    var_name = key.split('.')[0].split('/')[1]\n",
    "    print('Reading {0} features'.format(var_name))\n",
    "    exec(\"{0}=pd.read_csv(body, na_values = [r'\\\\\\\\N'])\".format(var_name))\n",
    "    exec(\"{0}.columns = {0}.columns.str.lower()\".format(var_name))\n",
    "    \n",
    "    # exclude the full null columns\n",
    "    exec(\"{0} = {0}.loc[:,{0}.isnull().sum()!={0}.shape[0]]\".format(var_name))\n",
    "    \n",
    "    # append the feature df\n",
    "    exec(\"data_list.append({0})\".format(var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a object\n",
    "logger.info('Setting up the prediction model')\n",
    "percentile_used = 0.8\n",
    "nfold = 10\n",
    "cv_func = ModelMain(data_list, metadata_process_info['label_columns'], metadata_process_info['num_columns'])\n",
    "# movie only\n",
    "#cv_func.df = cv_func.df.loc[cv_func.df['content_category']=='movies',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Title Prediction, Post Launch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Prediction\n",
    "Note: Because it is a post launch prediction, only the titles with partial percent view and view through portion data will be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the prediction tarjectory over length of data\n",
    "'''\n",
    "\n",
    "model_name = 'lgb'\n",
    "model_name_list = ['lgb']\n",
    "percent_data_process_info['exact_X_pred'] = False\n",
    "output_flag = False\n",
    "new_title_output = pd.DataFrame()\n",
    "existing_title_output = pd.DataFrame()\n",
    "\n",
    "for day in range(-27,27):\n",
    "    # renew the percent_data_process_info data very time\n",
    "    from lib.config import percent_data_process_info\n",
    "    from lib.config import prelaunch_process_info\n",
    "    from lib.config import metadata_process_info\n",
    "\n",
    "    # determine prelaunch or postlaunch\n",
    "    if day < 1:\n",
    "        input_process_info = dict(prelaunch_process_info)\n",
    "        input_percentile_used = 1\n",
    "    else:\n",
    "        input_process_info = dict(metadata_process_info)\n",
    "        input_percentile_used = percentile_used\n",
    "\n",
    "    # just to make the values in the dict back to the initial values\n",
    "    percent_data_process_info = dict(percent_data_process_info)\n",
    "    percent_data_process_info['max_num_day'] = day\n",
    "    \n",
    "    # get x and y\n",
    "    logger.info('Get X and y for day {}'.format(day))\n",
    "    cv_func.get_X_y(percent_data_process_info, \n",
    "                     input_process_info, \n",
    "                     day001_popularity_threshold = input_percentile_used)\n",
    "                     \n",
    "    if cv_func.pred_empty_flag == True:\n",
    "        print('no title needs to be predicted at day {}'.format(day))\n",
    "        continue\n",
    "\n",
    "    # tune parameter\n",
    "    logger.info('Tune parameter for day {}'.format(day))\n",
    "    print('Tune parameter for day {}'.format(day))\n",
    "    cv_func.parameter_tuning(model_name, \n",
    "                          params_tuning_dict, \n",
    "                          percent_data_process_info,\n",
    "                          nfold = nfold)\n",
    "    \n",
    "    params_dict = cv_func.min_smape_param['min_smape_original']\n",
    "    param_stats = cv_func.parameter_tuning_stats\n",
    "    logger.info('SMAPE for all titles {}'.format(param_stats['min_smape_all']))\n",
    "    logger.info('SMAPE for the originals {}'.format(param_stats['min_smape_original']))\n",
    "    print('SMAPE for all titles {}'.format(param_stats['min_smape_all']))\n",
    "    print('SMAPE for the originals {}'.format(param_stats['min_smape_original']))\n",
    "    \n",
    "    # make prediction\n",
    "    logger.info('Making prediction for day {}'.format(day))\n",
    "    print('Making prediction for day {}'.format(day))\n",
    "    cv_func.predict_new_titles(model_name_list, \n",
    "                               params_dict, \n",
    "                               percent_data_process_info)\n",
    "    \n",
    "    # process the output\n",
    "    cur_new_title_output = cv_func.new_title_output\n",
    "    pred_column = cur_new_title_output.columns[cur_new_title_output.columns.str.contains('lgb')][0]\n",
    "    cur_new_title_output['pred_day'] = day\n",
    "    cur_new_title_output = cur_new_title_output.rename(columns = {pred_column:'percent_view_pred'})\n",
    "    \n",
    "    # process the existing titles\n",
    "    cur_existing_title_output = cv_func.output\n",
    "    pred_column = cur_existing_title_output.columns[cur_existing_title_output.columns.str.contains('lgb')][0]\n",
    "    cur_existing_title_output['pred_day'] = day\n",
    "    cur_existing_title_output = cur_existing_title_output.rename(columns = {pred_column:'percent_view_pred'})\n",
    "    \n",
    "    if output_flag:\n",
    "        new_title_output = pd.concat([new_title_output,cur_new_title_output], axis = 0)\n",
    "        existing_title_output = pd.concat([existing_title_output, cur_existing_title_output], axis = 0)\n",
    "    else:\n",
    "        new_title_output = cur_new_title_output\n",
    "        existing_title_output = cur_existing_title_output\n",
    "        output_flag = True\n",
    "          \n",
    "# final formatting\n",
    "\n",
    "if new_title_output.shape[0]>0:    \n",
    "    new_title_output = new_title_output.drop(columns = ['target']).sort_values(['title_name','pred_day'])\n",
    "    new_title_output = new_title_output[['title_name'\n",
    "                                        ,'match_id'\n",
    "                                        ,'match_id_platform'\n",
    "                                        ,'platform_name'\n",
    "                                        ,'program_type'\n",
    "                                        ,'pred_day'\n",
    "                                        ,'percent_view_pred']]\n",
    "\n",
    "if existing_title_output.shape[0]>0:           \n",
    "    existing_title_output = existing_title_output.sort_values(['match_id_platform','pred_day'])\n",
    "    existing_title_output['platform_name'] = existing_title_output['match_id_platform'].apply(lambda x: x[0])\n",
    "    existing_title_output = existing_title_output[['title_name'\n",
    "                                                ,'match_id'\n",
    "                                                ,'match_id_platform'\n",
    "                                                ,'platform_name'\n",
    "                                                ,'program_type'\n",
    "                                                ,'target'\n",
    "                                                ,'pred_day'\n",
    "                                                ,'percent_view_pred'\n",
    "                                                ,'smape_lgb'\n",
    "                                                ,'mae_lgb'\n",
    "                                                ,'fold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write csvs to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write the prediction result to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3(filename, output_bucket, content):\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=filename, Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing new title predictions over time to S3 as an csv file')\n",
    "print('Writing new title predictions over time to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "new_title_output.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'output_percent_view/new_title_prediction.csv'\n",
    "\n",
    "to_s3(filename, output_bucket, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Writing existing title predictions over time to S3 as an csv file')\n",
    "print('Writing existing title predictions over time to S3 as an csv file')\n",
    "csv_buffer = io.StringIO()\n",
    "existing_title_output.to_csv(csv_buffer, index = False)\n",
    "content = csv_buffer.getvalue()\n",
    "\n",
    "filename = 'output_percent_view/existing_title_prediction.csv'\n",
    "\n",
    "to_s3(filename, output_bucket, content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('usr')",
   "name": "python373jvsc74a57bd05edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}