{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9350643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import date, datetime, timedelta\n",
    "import logging \n",
    "sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "from utils import * \n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "input_bucket=\"hbo-ingest-datascience-content-dev\"\n",
    "output_bucket=\"hbo-outbound-datascience-content-dev\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9507b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:TRAINING MODEL FOR 2021-12-01, Mode:test\n",
      "INFO:root:Read from psi_first_views/fv_train_2021-12-01.csv\n",
      "INFO:root:Read from psi_first_views/fv_train_imdb_2021-12-01.csv\n",
      "INFO:root:base_data shape: (270369, 18)\n",
      "INFO:root:base_data shape: (34160, 19)\n",
      "INFO:root:title imdb shape: (329, 10)\n",
      "INFO:root:Saved to psi_first_views/fv_decay_category_2021-12-01.csv\n",
      "INFO:root:Saved model to hbo-ingest-datascience-content-dev/psi_first_views/fv_2021-12-01.pkl\n",
      "INFO:root:Saved to psi_first_views/fv_2021-12-01.pkl\n",
      "INFO:root:Done model training 2021-12-01\n",
      "INFO:root:PREDICTING FOR 2021-12-01, Mode:predict\n",
      "INFO:root:Read from psi_first_views/fv_pred_2021-12-01.csv\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "INFO:root:base_data shape: (26821, 19)\n",
      "INFO:root:base_data shape: (26821, 8)\n",
      "INFO:root:title imdb shape: (833, 10)\n",
      "divide by zero encountered in log\n",
      "INFO:root:Read from psi_first_views/fv_2021-12-01.pkl\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "INFO:root:Read from psi_first_views/fv_decay_category_2021-12-01.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read from psi_first_views/fv_decay_category_2021-09-01.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content_category', 'tier', 'tier_adj', 'title_name',\n",
      "       'prequel_featured_count', 'season_number', 'prequel_count',\n",
      "       'effective_start_date', 'category', 'first_views_log_pred',\n",
      "       'first_views_pred', 'pred_date', 'decay_category', 'category_category',\n",
      "       'days_since_premiere', 'first_views_pct', 'first_views_pct_popcorn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join('psi_first_views', filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, filename):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = input_bucket\n",
    "        key = os.path.join('psi_first_views', filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved model to {os.path.join(bucket, key)}')\n",
    "        logger.info(f'Saved to {key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(filename):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = input_bucket\n",
    "        key = os.path.join('psi_first_views', filename)\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        logger.info(f'Read from {key}')\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(filename):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = input_bucket\n",
    "        key = os.path.join('psi_first_views', filename)\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        logger.info(f'Read from {key}')\n",
    "        return model\n",
    "\n",
    "\n",
    "class BaseFeatureMunger():\n",
    "    def __init__(self, df_in, df_imdb_munged, mode, date_run):\n",
    "        self.df_in = df_in\n",
    "        self.df_imdb_munged = df_imdb_munged\n",
    "        self.mode = mode\n",
    "        self.date_run = date_run\n",
    "        self.clean_df()\n",
    "        self.fill_missing_data()\n",
    "        if mode=='train':\n",
    "            self.filter_df()\n",
    "        self.get_datetime_features()\n",
    "        self.aggregate_df()\n",
    "        self.merge_imdb_features()\n",
    "        self.get_first_views()\n",
    "        self.return_df()\n",
    "        \n",
    "    def return_df(self):\n",
    "        col_base = ['title_id', 'title_name', 'season_number',\n",
    "       'content_category', 'content_source', 'category',\n",
    "       'tier', 'effective_start_date', 'request_date', 'first_views',\n",
    "       'days_since_premiere', 'days_on_platform', 'start_year']\n",
    "        col_title = ['tier','content_category','category','title_name', 'title_id','season_number',\n",
    "                'effective_start_date', 'prequel_count', 'prequel_featured_count','first_views','first_views_log']\n",
    "        if self.mode=='train':\n",
    "            return self.df_in[col_base], self.df_in_title[col_title]\n",
    "        elif self.mode=='predict':\n",
    "            return self.df_in_title[col_title]\n",
    "\n",
    "    def clean_df(self):\n",
    "        logger.info(f'base_data shape: {self.df_in.shape}')\n",
    "#         logger.info(f'base_data null: {self.df_in.isnull().sum()}')\n",
    "        self.dic_dtype = {'season_number':int, 'tier':int}\n",
    "        self.df_in = self.df_in.astype(self.dic_dtype)\n",
    "        self.df_in['effective_start_date'] = pd.to_datetime(self.df_in['effective_start_date'])\n",
    "        \n",
    "    def fill_missing_data(self):\n",
    "        self.df_in['content_category'] = self.df_in['content_category'].fillna('series')\n",
    "        self.df_in.loc[self.df_in['category'].isin(['Specials']), 'content_category']='special'\n",
    "        self.df_in.loc[self.df_in['category'].isin(['Popcorn','Pay 1 WB Theatricals','Scripted Features', 'Pay1']), 'content_category']='movies'\n",
    "        self.df_in.loc[self.df_in['title_name'].str.contains('Harry Potter'), 'content_category']='special'\n",
    "        self.df_in = self.df_in.fillna(0)\n",
    "\n",
    "    def filter_df(self):\n",
    "        self.date_max = datetime.strptime(self.date_run, '%Y-%m-%d')- timedelta(days=60)\n",
    "        self.df_in = self.df_in[(self.df_in['effective_start_date']<=self.date_max)]\n",
    "\n",
    "    def get_datetime_features(self):\n",
    "        self.df_in['start_year'] = self.df_in['effective_start_date'].dt.year\n",
    "        self.df_in['start_quarter'] = self.df_in['effective_start_date'].dt.quarter\n",
    "        \n",
    "    def aggregate_df(self):\n",
    "        grpby_title= ['title_name', 'title_id','tier','content_category','category','season_number',\n",
    "                'effective_start_date']\n",
    "        self.df_in_title = self.df_in[grpby_title + ['first_views']].groupby(by=grpby_title).sum().reset_index()\n",
    "\n",
    "    def merge_imdb_features(self):\n",
    "        key_merge = ['title_name','season_number', 'content_category', 'category', 'tier', 'effective_start_date']\n",
    "        self.df_in_title = self.df_in_title.merge(self.df_imdb_munged, \n",
    "                                         on= key_merge, how='left')\n",
    "        logger.info(f'title imdb shape: {self.df_in_title.shape}')\n",
    "#         logger.info(f'title imdb features null: {self.df_in_title.isnull().sum()}')\n",
    "        self.df_in_title = self.df_in_title.fillna(0)\n",
    "        \n",
    "    def get_first_views(self):\n",
    "        ## Scale first_views \n",
    "        self.df_in_title['first_views_log'] = np.log(self.df_in_title['first_views']) \n",
    "        self.df_in_title.loc[(self.df_in_title.first_views==0), 'first_views_log']=0\n",
    "        \n",
    "\n",
    "class IMDBFeatureMunger(BaseFeatureMunger):\n",
    "    \n",
    "    def __init__(self, df_in, mode, date_run):\n",
    "        self.df_in = df_in\n",
    "        self.date_run = date_run \n",
    "        \n",
    "        self.clean_df()\n",
    "        if mode=='train':\n",
    "            self.filter_df()\n",
    "        self.get_series_features()\n",
    "        if mode=='train':\n",
    "            self.adjust_series_for_training_data()\n",
    "        self.get_non_series_prequel_features()\n",
    "        self.get_non_series_prequel_ref_features()\n",
    "        self.merge_non_series_features()\n",
    "        self.return_df()\n",
    "        \n",
    "    def return_df(self):\n",
    "        col_imdb = ['title_name', 'season_number', 'content_category', 'category', \n",
    "                    'tier', 'effective_start_date', 'prequel_count', 'prequel_featured_count']\n",
    "        self.df_imdb_munged = pd.concat([self.df_series_title, self.df_nseries_title])\n",
    "        self.df_imdb_munged = self.df_imdb_munged[col_imdb]\n",
    "        return self.df_imdb_munged\n",
    "    \n",
    "    def get_series_features(self):\n",
    "        grpby_series = ['title_name','title_id','season_number','content_category','category','tier','effective_start_date']\n",
    "        self.df_series = self.df_in[(self.df_in.content_category=='series') & (self.df_in.reference_type.isin(['featured_in']))] \n",
    "        self.df_series_title = self.df_series.groupby(by=grpby_series).agg({'reference_title_id':'nunique'}).reset_index()\n",
    "        self.df_series_title = self.df_series_title.rename(columns={'reference_title_id':'ref_ref_featured_in'})\n",
    "#         self.df_series_title[['ref_follows','ref_spin_off_from','ref_remake_of']] = 0\n",
    "        self.df_series_title['prequel_count'] = self.df_series_title['season_number']\n",
    "\n",
    "    def adjust_series_for_training_data(self):     \n",
    "        self.df_series_title['ref_ref_featured_in'] = (self.df_series_title['ref_ref_featured_in'] * (self.df_series_title['season_number']-1))/self.df_series_title['season_number']\n",
    "        self.df_series_title.loc[self.df_series_title.season_number>1, 'prequel_featured_count'] = (self.df_series_title['ref_ref_featured_in'])/(self.df_series_title['season_number']-1)\n",
    "\n",
    "    def get_non_series_prequel_features(self):\n",
    "        self.grpby_nseries=['title_name','title_id','season_number','content_category','category','tier','effective_start_date','imdb_imdb_series_id']\n",
    "        \n",
    "        self.df_nseries = self.df_in[(self.df_in.content_category!='series')\\\n",
    "                                     & (self.df_in.reference_type.isin(['follows','spin_off_from','remake_of','version_of']))] \n",
    "        self.df_nseries_preq = self.df_nseries.groupby(by=self.grpby_nseries+['reference_type']).agg({'reference_title_id':'nunique'}).reset_index()\n",
    "        self.df_nseries_preq = self.df_nseries_preq.pivot(index=self.grpby_nseries, \n",
    "                                                                columns='reference_type', \n",
    "                                                                values='reference_title_id').reset_index()\n",
    "        self.df_nseries_preq = self.df_nseries_preq.rename(columns={'follows':'ref_follows',  \n",
    "                                                                          'spin_off_from':'ref_spin_off_from', \n",
    "                                                                          'remake_of':'ref_remake_of',\n",
    "                                                                          'version_of':'ref_version_of'})\n",
    "        \n",
    "    def get_non_series_prequel_ref_features(self):\n",
    "        self.df_nseries_preq_ref = self.df_nseries.groupby(by=self.grpby_nseries+['reference_reference_type']).agg({'reference_reference_title_id':'nunique'}).reset_index()\n",
    "        self.df_nseries_preq_ref = self.df_nseries_preq_ref.pivot(index=self.grpby_nseries, \\\n",
    "                                                                        columns='reference_reference_type', values='reference_reference_title_id').reset_index()\n",
    "        self.df_nseries_preq_ref = self.df_nseries_preq_ref.rename(columns={'featured_in':'ref_ref_featured_in'})\n",
    "\n",
    "    def merge_non_series_features(self):\n",
    "        col_preq = ['title_name', 'tier','season_number','content_category','category','effective_start_date','imdb_imdb_series_id','ref_follows','ref_spin_off_from','ref_remake_of', 'ref_version_of']\n",
    "        col_ref_ref = ['title_name','season_number','category','ref_ref_featured_in']\n",
    "        self.df_nseries_title = self.df_nseries_preq[col_preq].merge(self.df_nseries_preq_ref[col_ref_ref], how='outer', on=['title_name','season_number','category'])\n",
    "        self.df_nseries_title['prequel_count'] = self.df_nseries_title[['ref_follows','ref_spin_off_from','ref_remake_of','ref_version_of']].sum(axis=1)\n",
    "        self.df_nseries_title['prequel_featured_count'] = self.df_nseries_title['ref_ref_featured_in']/self.df_nseries_title['prequel_count']\n",
    "        \n",
    "        \n",
    "class PreProcessor():\n",
    "    def __init__(self, df_in, mode, date_run):\n",
    "        self.df_in = df_in\n",
    "        self.date_run = date_run\n",
    "        self.get_parameters()\n",
    "        self.winsorize_features()\n",
    "        self.adjust_tiers()\n",
    "        if mode=='train':\n",
    "            self.winsorize_label()\n",
    "            self.resample_data()\n",
    "        self.return_df()\n",
    "        \n",
    "    def return_df(self):\n",
    "        return self.df_in\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        self.max_firstviews = 1000000\n",
    "        self.old_sampling_rate = 0.25\n",
    "        self.recent_sampling_rate = 0.75\n",
    "        self.popcorn_old_sampling_rate = 0.2\n",
    "        self.popcorn_recent_sampling_rate = 0.3\n",
    "    \n",
    "    def adjust_tiers(self):\n",
    "        self.df_in['tier_adj'] = self.df_in['tier']\n",
    "        self.df_in.loc[(self.df_in.title_name=='Reminiscence'), 'tier_adj'] = 2\n",
    "        self.df_in.loc[(self.df_in.category=='Pay1') & (self.df_in.tier==1), 'tier_adj'] = 2\n",
    "        self.df_in.loc[(self.df_in.category=='Pay1') & (self.df_in.tier==2), 'tier_adj'] = 3\n",
    "#         self.df_in.loc[(self.df_in.effective_start_date < '2022-04-01') & (self.df_in.category=='Popcorn') & (self.df_in.tier==1), 'tier_adj'] = 0\n",
    "#         self.df_in.loc[(self.df_in.effective_start_date < '2022-04-01') & (self.df_in.category=='Popcorn') & (self.df_in.tier==2), 'tier_adj'] = 1\n",
    "        \n",
    "    def winsorize_label(self):\n",
    "        self.df_in.loc[self.df_in.first_views>self.max_firstviews, 'first_views'] = self.max_firstviews\n",
    "    \n",
    "    def winsorize_features(self):\n",
    "        ## winsorize season to 10\n",
    "        ## winsorize ref_tot to 5 \n",
    "        ## winsorize ref_ref tot to 20\n",
    "        ## Penalize kids & family \n",
    "        ## for series future titles, nullify ref_ features to keep consistent \n",
    "        self.df_in.loc[(self.df_in['prequel_count']>5), ['prequel_count']] = 5\n",
    "        self.df_in.loc[(self.df_in['prequel_featured_count']>20), ['prequel_featured_count']] = 20   \n",
    "#         self.df_in.loc[(self.df_in.content_category=='series') & (self.df_in['prequel_count']>3), 'prequel_count'] = 3\n",
    "#         self.df_in.loc[(self.df_in['category']=='Kids & Family'), ['prequel_featured_count']] = -1\n",
    "        self.df_in.loc[(self.df_in['content_category']=='series'), ['prequel_count']] = -1\n",
    "        self.df_in.loc[(self.df_in['content_category']=='series'), ['prequel_featured_count']] = -1\n",
    "       \n",
    "    def resample_data(self):  \n",
    "        self.df_in_old = self.df_in[(self.df_in.effective_start_date < datetime.strptime(self.date_run, '%Y-%m-%d')- timedelta(days=120))]\n",
    "        self.df_in_recent = self.df_in[(self.df_in.effective_start_date >= datetime.strptime(self.date_run, '%Y-%m-%d')- timedelta(days=120))]\n",
    "        target_count = self.df_in[self.df_in.tier==3].shape[0]\n",
    "        target_count_old = int(round(target_count*self.old_sampling_rate))\n",
    "        target_count_new = int(round(target_count*self.recent_sampling_rate))\n",
    "        \n",
    "        df_resample_list = []\n",
    "        \n",
    "        ## resample old data. try/except in place in case categories are unavailable  \n",
    "        for i in [0,1,2]:\n",
    "            try:\n",
    "                df_resample_list.append(self.df_in_old[(self.df_in_old.tier==i) & (self.df_in_old.category!='Popcorn')].sample(target_count_old, replace=True))\n",
    "            except:\n",
    "                pass\n",
    "            try:    \n",
    "                df_resample_list.append(self.df_in_recent[(self.df_in_recent.tier==i) & (self.df_in_recent.category!='Popcorn')].sample(target_count_new, replace=True))\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            df_resample_list.append(self.df_in_old[(self.df_in_old.category=='Popcorn')].sample(int(round(target_count_old*self.popcorn_old_sampling_rate)), replace=True))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_resample_list.append(self.df_in_recent[(self.df_in_recent.category=='Popcorn')].sample(int(round(target_count_new*self.popcorn_recent_sampling_rate)), replace=True))\n",
    "        except:\n",
    "            pass\n",
    "        df_resample_list.append(self.df_in_old[(self.df_in_old.tier==3) & (self.df_in_old.category!='Popcorn')])\n",
    "        df_resample_list.append(self.df_in_recent[(self.df_in_recent.tier==3) & (self.df_in_recent.category!='Popcorn')])\n",
    "        self.df_in = pd.concat(df_resample_list, axis=0)\n",
    "    \n",
    "    \n",
    "class XGB(Utils):\n",
    "    def __init__(self, df_in, mode, date_run):\n",
    "        self.df_in = df_in    \n",
    "        self.mode = mode\n",
    "        self.date_run = date_run\n",
    "        self.get_parameters()\n",
    "        \n",
    "        if mode=='train':\n",
    "            self.train_xgb()\n",
    "            self.save_model()\n",
    "        elif mode=='predict':\n",
    "            self.get_model()\n",
    "            self.predict_xgb()\n",
    "            self.return_df()\n",
    "            \n",
    "    @staticmethod\n",
    "    def _encode(df, categoricals):\n",
    "        \n",
    "        \"\"\"\n",
    "        perform category encoding on the data\n",
    "        :param df: dataframe to be encoded\n",
    "        :param categoricals: list of name of categorical columns\n",
    "        :return ohe, x_ohe: OHE object and OHE-encoded data\n",
    "        \"\"\"\n",
    "        ohe = OneHotEncoder(cols=categoricals, \n",
    "                            handle_unknown='return_nan',\n",
    "                           handle_missing='return_nan',  \n",
    "                           use_cat_names=True) \n",
    "        x_ohe = ohe.fit_transform(df)\n",
    "        return ohe, x_ohe\n",
    "    \n",
    "    def return_df(self):\n",
    "        return self.df_pred\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        self.target = 'first_views_log'\n",
    "        self.features_cat=['tier_adj','category','content_category','prequel_count'] #\n",
    "        self.features_cont=['prequel_featured_count']\n",
    "        self.param_xgb = {\"booster\":\"gbtree\",\n",
    "                     \"objective\":\"reg:squarederror\",\n",
    "                    \"gamma\":1}\n",
    "\n",
    "    def train_xgb(self):\n",
    "        x_train = self.df_in[self.features_cat + self.features_cont]\n",
    "        y_train = self.df_in[self.target]\n",
    "        self.ohe, x_ohe = self._encode(x_train, self.features_cat)\n",
    "        dm_train = xgb.DMatrix(x_ohe, label=y_train)\n",
    "\n",
    "        ## train \n",
    "        self.model = xgb.train(params = self.param_xgb, dtrain = dm_train, num_boost_round = 10)\n",
    "        \n",
    "    def save_model(self):\n",
    "        dict_model = {'model': self.model, 'ohe': self.ohe}\n",
    "        Utils.to_pkl_s3(dict_model, f'fv_{date_train}.pkl')\n",
    "        logger.info(f'Done model training {date_train}')\n",
    "    \n",
    "    def get_model(self):\n",
    "        dict_model = Utils.read_pkl_s3(f'fv_{date_train}.pkl')\n",
    "        self.ohe = dict_model['ohe']\n",
    "        self.model = dict_model['model']\n",
    "        \n",
    "    def predict_xgb(self):\n",
    "        x_test = self.df_in[self.features_cat + self.features_cont]\n",
    "        x_ohe_test = self.ohe.transform(x_test)\n",
    "        dm_test = xgb.DMatrix(x_ohe_test)\n",
    "        pred = self.model.predict(dm_test)\n",
    "\n",
    "        self.df_pred = self.df_in[list(set(['title_name','category','season_number', 'effective_start_date', 'tier','tier_adj'] + self.features_cat + self.features_cont))]\n",
    "        self.df_pred['first_views_log_pred'] = pred\n",
    "        self.df_pred[f'first_views_pred'] = np.exp(self.df_pred[f'first_views_log_pred'])\n",
    "        self.df_pred['pred_date'] = self.date_run\n",
    "        self.df_pred['pred_date'] = pd.to_datetime(self.df_pred['pred_date'])\n",
    "        self.df_pred.loc[(self.df_pred.category=='Popcorn') & (self.df_pred.effective_start_date>='2022-04-01'), 'category']='Pay 1 WB Theatricals'\n",
    "\n",
    "\n",
    "class FVDecay(Utils):\n",
    "    def __init__(self, df_in, mode, date_run, grpby=''):\n",
    "        self.df_in = df_in    \n",
    "        self.mode = mode\n",
    "        self.date_run = date_run\n",
    "        self.grpby = grpby\n",
    "\n",
    "        if mode=='train':\n",
    "            self.get_decay_data_title()\n",
    "            self.get_decay_curve()\n",
    "            self.save_decay_curve()\n",
    "            self.return_df()\n",
    "\n",
    "    def return_df(self):\n",
    "        return self.df_decay\n",
    "        \n",
    "    @staticmethod\n",
    "    def apply_decay_curve(df_in, df_decay, df_popcorn_decay):\n",
    "        category_list_train = df_decay.category.unique().tolist()\n",
    "        df_in['decay_category'] = df_in['category']\n",
    "        df_in.loc[(~df_in['decay_category'].isin(category_list_train)) & (df_in['decay_category']!='Popcorn'), 'decay_category'] = 'Pay1'\n",
    "        \n",
    "        \n",
    "        ## Apply decay to prediction \n",
    "        df_pred_decay = df_in.merge(df_decay[['category','days_since_premiere','first_views_pct']], \n",
    "                                                left_on=['decay_category'], right_on=['category'], how='left', suffixes=['', '_category'])\\\n",
    "                            .merge(df_popcorn_decay[['category','days_since_premiere','first_views_pct']], \n",
    "                                        on=['category','days_since_premiere'], how='left', suffixes=['', '_popcorn'])\n",
    "        print(df_pred_decay.columns)\n",
    "        df_pred_decay.loc[(df_pred_decay.category=='Popcorn') & (df_pred_decay.first_views_pct.isnull()), 'first_views_pct'] = df_pred_decay['first_views_pct_popcorn']\n",
    "        \n",
    "        df_pred_decay[f'first_views_pred_decay'] = df_pred_decay[f'first_views_pred'] * df_pred_decay['first_views_pct']\n",
    "        df_pred_decay['start_month'] = df_pred_decay['effective_start_date'].dt.to_period('M').dt.to_timestamp()\n",
    "        df_pred_decay['request_date'] = df_pred_decay['effective_start_date'] + pd.to_timedelta(df_pred_decay.days_since_premiere, unit=\"D\")\n",
    "        df_pred_decay['first_view_quarter'] = df_pred_decay['request_date'].dt.to_period('Q').dt.to_timestamp()\n",
    "        df_pred_decay['first_view_month'] = df_pred_decay['request_date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "        return df_pred_decay\n",
    "    \n",
    "    def get_decay_data_title(self):\n",
    "        self.key_col = ['title_name','tier','content_category','category','season_number','effective_start_date']\n",
    "        self.df_in = self.df_in[self.key_col + ['title_id','days_since_premiere','first_views']].sort_values(by=self.key_col+['days_since_premiere'])\n",
    "        self.df_in['first_views_sum'] = self.df_in.groupby(self.key_col)['first_views'].transform('sum')\n",
    "        self.df_in['first_views_cumsum'] = self.df_in.groupby(by=self.key_col)['first_views'].cumsum()\n",
    "        self.df_in['first_views_cumpct'] = self.df_in['first_views_cumsum'] / self.df_in['first_views_sum']\n",
    "        self.df_in.reset_index(inplace=True)\n",
    "            \n",
    "    def get_decay_curve(self):\n",
    "        self.df_decay = self.df_in[[self.grpby] + ['title_id','days_since_premiere','first_views_cumpct']].sort_values(by=[self.grpby]+['days_since_premiere'])\n",
    "        self.df_decay = self.df_decay.groupby(by=[self.grpby] + ['days_since_premiere']).agg({'first_views_cumpct':'median','title_id':'nunique'})\n",
    "        self.df_decay = self.df_decay.rename(columns={'title_id':'title_count_training'})\n",
    "        self.df_decay = self.df_decay.reset_index()\n",
    "        self.df_decay['first_views_pct'] = self.df_decay.groupby(self.grpby)['first_views_cumpct'].diff()\n",
    "        self.df_decay.reset_index(inplace=True)\n",
    "        \n",
    "        self.df_decay['days_since_premiere'] = self.df_decay['days_since_premiere'].astype(int)\n",
    "        self.df_decay.loc[(self.df_decay.days_since_premiere==0),'first_views_pct'] = self.df_decay['first_views_cumpct']\n",
    "        \n",
    "    def save_decay_curve(self):\n",
    "        Utils.to_csv_s3(self.df_decay, input_bucket, f'fv_decay_{self.grpby}_{self.date_run}.csv')\n",
    "\n",
    "mode = 'test' ## 'prod', 'test', 'backtest'\n",
    "df_pred_list = []\n",
    "df_pred_decay_list = [] ##\n",
    "if mode=='backtest':\n",
    "    list_date_train = ['2021-01-01', '2021-02-01', '2021-03-01','2021-04-01', '2021-05-01', '2021-06-01','2021-07-01', '2021-08-01', '2021-09-01']\n",
    "elif mode =='test':\n",
    "    list_date_train = ['2021-12-01']\n",
    "elif mode =='prod':\n",
    "    list_date_train = [(datetime.today()).strftime('%Y-%m-%d')]\n",
    "    \n",
    "for date_train in list_date_train:\n",
    "    date_pred = date_train \n",
    "    logger.info(f'TRAINING MODEL FOR {date_train}, Mode:{mode}')\n",
    "\n",
    "    #### Train \n",
    "    mode = 'train'\n",
    "    key_col = ['title_name', 'title_id','tier','content_category','category','season_number','effective_start_date']\n",
    "\n",
    "    ## Read data \n",
    "    df_raw = Utils.read_csv_s3(f'fv_train_fv_{date_train}.csv')\n",
    "    df_imdb = Utils.read_csv_s3(f'fv_train_imdb_{date_train}.csv')\n",
    "\n",
    "    ## Munge features \n",
    "    df_imdb_munged = IMDBFeatureMunger(df_imdb, mode, date_train).df_imdb_munged\n",
    "    df_base_munged_decay, df_base_munged = BaseFeatureMunger(df_raw, df_imdb_munged, mode, date_train).return_df()\n",
    "    df_decay_category = FVDecay(df_base_munged_decay, mode, date_train, 'category').return_df()\n",
    "\n",
    "    df_in_train = PreProcessor(df_base_munged, mode, date_train).return_df()  \n",
    "    XGB(df_in_train, mode, date_train)\n",
    "\n",
    "\n",
    "    #### Predict \n",
    "    mode = 'predict'\n",
    "    logger.info(f'PREDICTING FOR {date_pred}, Mode:{mode}')\n",
    "    \n",
    "    ## Read data and munge features \n",
    "    if mode=='backtest':\n",
    "        df_imdb_pred = Utils.read_csv_s3(f'fv_train_imdb_2021-12-01.csv')\n",
    "        df_raw_pred = Utils.read_csv_s3(f'fv_train_2021-12-15.csv')\n",
    "        df_imdb_munged_pred = IMDBFeatureMunger(df_imdb_pred, mode, date_pred).return_df()\n",
    "        df_base_munged_pred = BaseFeatureMunger(df_raw_pred, df_imdb_munged_pred, mode, date_pred).return_df()\n",
    "        df_base_munged_pred = df_base_munged_pred[(df_base_munged_pred.effective_start_date >= date_pred)\\\n",
    "                                  & (df_base_munged_pred.effective_start_date < datetime.strptime(date_pred, '%Y-%m-%d')+ timedelta(days=92))]\n",
    "    else:\n",
    "        df_imdb_pred = Utils.read_csv_s3(f'fv_pred_{date_pred}.csv')\n",
    "        df_raw_pred = df_imdb_pred[key_col]\n",
    "        df_raw_pred['first_views'] = 0\n",
    "        df_imdb_munged_pred = IMDBFeatureMunger(df_imdb_pred, mode, date_pred).return_df()\n",
    "        df_base_munged_pred = BaseFeatureMunger(df_raw_pred, df_imdb_munged_pred, mode, date_pred).return_df()\n",
    "        \n",
    "#     print(df_base_munged_pred[df_base_munged_pred.title_name.str.contains('Curb')].season_number.unique())\n",
    "    df_in_pred = PreProcessor(df_base_munged_pred, mode, date_pred).return_df()  \n",
    "    print(df_in_pred[df_in_pred.title_name.str.contains('Curb')].season_number.unique())\n",
    "    ## Predict and apply decay \n",
    "    df_out_pred = XGB(df_in_pred, mode, date_pred).return_df()\n",
    "    df_decay_category = Utils.read_csv_s3(f'fv_decay_category_{date_train}.csv') \n",
    "    df_decay_popcorn = Utils.read_csv_s3(f'fv_decay_category_2021-09-01.csv') \n",
    "    df_pred_decay = FVDecay.apply_decay_curve(df_out_pred, df_decay_category, df_decay_popcorn)\n",
    "    \n",
    "    df_pred_list.append(df_out_pred)\n",
    "    df_pred_decay_list.append(df_pred_decay)\n",
    "\n",
    "df_pred = pd.concat(df_pred_list)\n",
    "df_pred_decay = pd.concat(df_pred_decay_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c0d9514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_name</th>\n",
       "      <th>first_view_month</th>\n",
       "      <th>premiere_date</th>\n",
       "      <th>season_number</th>\n",
       "      <th>tier</th>\n",
       "      <th>content_category</th>\n",
       "      <th>category</th>\n",
       "      <th>prequel_count</th>\n",
       "      <th>prequel_featured_count</th>\n",
       "      <th>tier_adjusted</th>\n",
       "      <th>first_views_pred</th>\n",
       "      <th>model_train_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>17440.642578</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>108131.992188</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>104643.859375</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>87203.218750</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Dates of Christmas</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>series</td>\n",
       "      <td>Unscripted Series</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>22811.865234</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title_name first_view_month premiere_date  season_number  tier  \\\n",
       "0          100 Foot Wave       2022-07-01    2022-07-27              2     3   \n",
       "1          100 Foot Wave       2022-08-01    2022-07-27              2     3   \n",
       "2          100 Foot Wave       2022-09-01    2022-07-27              2     3   \n",
       "3          100 Foot Wave       2022-10-01    2022-07-27              2     3   \n",
       "4  12 Dates of Christmas       2021-11-01    2021-11-25              2     3   \n",
       "\n",
       "  content_category           category  prequel_count  prequel_featured_count  \\\n",
       "0           series        Docu-Series           -1.0                    -1.0   \n",
       "1           series        Docu-Series           -1.0                    -1.0   \n",
       "2           series        Docu-Series           -1.0                    -1.0   \n",
       "3           series        Docu-Series           -1.0                    -1.0   \n",
       "4           series  Unscripted Series           -1.0                    -1.0   \n",
       "\n",
       "   tier_adjusted  first_views_pred model_train_date  \n",
       "0              3      17440.642578       2021-12-01  \n",
       "1              3     108131.992188       2021-12-01  \n",
       "2              3     104643.859375       2021-12-01  \n",
       "3              3      87203.218750       2021-12-01  \n",
       "4              3      22811.865234       2021-12-01  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_future_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0195beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved to psi_first_views/future_program_xgb_pred.csv\n",
      "INFO:root:Saved to psi_first_views/future_program_xgb_pred_2021-12-01.csv\n"
     ]
    }
   ],
   "source": [
    "def get_agg_first_views(df_in, agg_col, grpby_title, first_view_date_col, first_view_col):\n",
    "    grpby_title_agg = grpby_title + [agg_col]\n",
    "    df_in[first_view_date_col] = pd.to_datetime(df_in[first_view_date_col])\n",
    "    df_in['first_view_quarter'] = df_in[first_view_date_col].dt.to_period(\"Q\").dt.to_timestamp()   \n",
    "    df_in['first_view_month'] = df_in[first_view_date_col].dt.to_period('M').dt.to_timestamp()\n",
    "    df_in = df_in[grpby_title_agg + first_view_col].groupby(by=grpby_title_agg).sum().reset_index()\n",
    "    return df_in\n",
    "\n",
    "### Save to csv     \n",
    "dic_rename = {'tier_adj':'tier_adjusted', 'effective_start_date':'premiere_date', 'pred_date':'model_train_date'} \n",
    "out_col = ['title_name', 'first_view_month', 'premiere_date', 'season_number',\n",
    "           'tier','content_category', 'category', 'prequel_count', 'prequel_featured_count',\n",
    "           'tier_adjusted', 'first_views_pred', 'model_train_date']\n",
    "agg_var = 'first_view_month'\n",
    "\n",
    "df_pred_future_out = df_pred_decay.rename(columns=dic_rename)\n",
    "df_pred_future_out = df_pred_future_out[out_col].groupby(by=out_col[:-2]+['model_train_date']).sum().reset_index()\n",
    "df_pred_future_out = df_pred_future_out[out_col]\n",
    "\n",
    "Utils.to_csv_s3(df_pred_future_out, output_bucket, f'future_program_xgb_pred.csv')\n",
    "Utils.to_csv_s3(df_pred_future_out, input_bucket, f'future_program_xgb_pred_{date_train}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1550f1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_name</th>\n",
       "      <th>season_number</th>\n",
       "      <th>content_category</th>\n",
       "      <th>category</th>\n",
       "      <th>tier</th>\n",
       "      <th>premiere_date</th>\n",
       "      <th>first_view_month</th>\n",
       "      <th>imdb_prequel_count</th>\n",
       "      <th>imdb_prequel_references</th>\n",
       "      <th>tier_adjusted</th>\n",
       "      <th>category_adjusted</th>\n",
       "      <th>first_views_pred</th>\n",
       "      <th>model_train_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>900.102632</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>2654.441796</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>576.575886</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100 Foot Wave</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Docu-Series</td>\n",
       "      <td>184.206834</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Dates of Christmas</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>Unscripted Series</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Unscripted Series</td>\n",
       "      <td>1211.584539</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title_name  season_number content_category           category  \\\n",
       "0          100 Foot Wave              2           series        Docu-Series   \n",
       "1          100 Foot Wave              2           series        Docu-Series   \n",
       "2          100 Foot Wave              2           series        Docu-Series   \n",
       "3          100 Foot Wave              2           series        Docu-Series   \n",
       "4  12 Dates of Christmas              2           series  Unscripted Series   \n",
       "\n",
       "  tier premiere_date first_view_month  imdb_prequel_count  \\\n",
       "0    3    2022-07-27       2022-07-01                 0.0   \n",
       "1    3    2022-07-27       2022-08-01                 0.0   \n",
       "2    3    2022-07-27       2022-09-01                 0.0   \n",
       "3    3    2022-07-27       2022-10-01                 0.0   \n",
       "4    3    2021-11-25       2021-11-01                 0.0   \n",
       "\n",
       "   imdb_prequel_references  tier_adjusted  category_adjusted  \\\n",
       "0                      0.0              3        Docu-Series   \n",
       "1                      0.0              3        Docu-Series   \n",
       "2                      0.0              3        Docu-Series   \n",
       "3                      0.0              3        Docu-Series   \n",
       "4                      0.0              3  Unscripted Series   \n",
       "\n",
       "   first_views_pred model_train_date  \n",
       "0        900.102632       2021-09-01  \n",
       "1       2654.441796       2021-09-01  \n",
       "2        576.575886       2021-09-01  \n",
       "3        184.206834       2021-09-01  \n",
       "4       1211.584539       2021-09-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-02 00:00:00 2024-12-19 00:00:00\n",
      "(725, 9) 591\n",
      "Create Table: future_program_xgb_pred\n",
      "Begin Uploading\n",
      "Finish Uploading\n"
     ]
    }
   ],
   "source": [
    "# ### Publish to output table \n",
    "\n",
    "\n",
    "# def cvdf_to_snowflake(df, table_name):\n",
    "#     stage = '@HBO_OUTBOUND_DATASCIENCE_CONTENT_DEV'\n",
    "#     output_bucket = \"hbo-outbound-datascience-content-dev\"\n",
    "#     filename ='psi/' + table_name + '.csv'\n",
    "#     dbname, schema = 'MAX_DEV', 'WORKSPACE'\n",
    "    \n",
    "#     csv_buffer = io.StringIO()\n",
    "#     df.to_csv(csv_buffer, index = False)\n",
    "#     content = csv_buffer.getvalue()\n",
    "#     client = boto3.client('s3')\n",
    "#     client.put_object(Bucket=output_bucket, Key=filename, Body=content)\n",
    "\n",
    "#     print ('Create Table: ' + table_name)\n",
    "#     run_query('''\n",
    "#     create or replace table {table_name}(\n",
    "#     title_name varchar,\n",
    "#     season_number int, \n",
    "#     content_category  varchar,\n",
    "#     category varchar,\n",
    "#     tier varchar,\n",
    "#     premiere_date varchar,\n",
    "#     first_view_month varchar,\n",
    "#     imdb_prequel_count int,\n",
    "#     imdb_prequel_references int,\n",
    "#     tier_adjusted int,\n",
    "#     category_adjusted varchar,\n",
    "#     first_views_pred float,\n",
    "#     model_train_date varchar\n",
    "#     )\n",
    "#     '''.format(table_name = table_name), dbname, schema)\n",
    "\n",
    "#     print ('Begin Uploading')\n",
    "#     run_query('''\n",
    "#     insert into max_dev.workspace.{table_name}\n",
    "\n",
    "#     select \n",
    "#           $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13   \n",
    "#     from {stage}/psi/{file_name}\n",
    "\n",
    "#      (FILE_FORMAT => csv_v2)\n",
    "\n",
    "#     '''.format(stage = stage, table_name = table_name,\n",
    "#               file_name = table_name+'.csv')\n",
    "#             , dbname, schema)\n",
    "\n",
    "#     print ('Finish Uploading')    \n",
    "\n",
    "\n",
    "# def run_query(query, dbname, schema):\n",
    "#     SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "#     conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "#     ctx=conn.connect(dbname,schema)\n",
    "#     cursor = ctx.cursor()\n",
    "#     cursor.execute(query)\n",
    "#     df = pd.DataFrame(cursor.fetchall(), columns = [desc[0] for desc in cursor.description])\n",
    "#     df.columns= df.columns.str.lower()\n",
    "#     return df\n",
    "\n",
    "\n",
    "    \n",
    "# dic_rename = {'tier_new':'tier_adjusted', 'effective_start_date':'premiere_date', \n",
    "#               'category':'category_adjusted','category_original':'category', \n",
    "#               'ref_tot':'imdb_prequel_count', 'ref_ref_tot':'imdb_prequel_references',\n",
    "#              'first_views_decay_dev_b':'first_views_pred'}\n",
    "\n",
    "# out_col = ['title_name',  'season_number','content_category', 'category',\n",
    "#            'tier', 'premiere_date','first_view_month', \n",
    "#            'imdb_prequel_count', 'imdb_prequel_references', 'tier_adjusted', 'category_adjusted','first_views_pred', 'model_train_date']\n",
    "\n",
    "\n",
    "# grpby_title= out_col.copy()\n",
    "# agg_var = 'first_view_month'\n",
    "# grpby_title.remove('first_views_pred')\n",
    "# grpby_title.remove(agg_var)\n",
    "\n",
    "# df_pred_future_out = df_pred_decay_future.rename(columns=dic_rename)\n",
    "# df_pred_future_out = get_agg_first_views(df_pred_future_out, agg_var, grpby_title,'request_date', ['first_views_pred'])\n",
    "# df_pred_future_out = df_pred_future_out[out_col]\n",
    "# ## QA \n",
    "# display(df_pred_future_out.head())\n",
    "# a = df_pred_future_out.groupby(by=['title_name','category','tier','premiere_date']).sum().reset_index()\n",
    "# a[a.category=='Pay 1 WB Theatricals']\n",
    "\n",
    "# print(a.premiere_date.min(),a.premiere_date.max()) \n",
    "# print(a.shape, a.title_name.nunique())\n",
    "\n",
    "# ## Upload to snowflake \n",
    "\n",
    "# import io\n",
    "# df_pred_future_out.to_csv('s3://datascience-hbo-users/users/tjung/psi/future_program_xgb_pred.csv')\n",
    "# cvdf_to_snowflake(df_pred_future_out, 'future_program_xgb_pred')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
